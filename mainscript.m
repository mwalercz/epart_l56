% load data and remap labels[tvec tlab tstv tstl] = readSets(); tlab += 1;tstl += 1;% we have to implement properly activation and its derivative functionsx = -5:0.1:5;y = actf(x);plot(x, actf(x))% note that input to derivative function is not x (input value of the neuron) but the OUTPUT valueplot(x, actdf(y))% in implementation of the backprop I want to use very small training set[mu trmx] = prepTransform(tvec, 40);tvec40 = pcaTransform(tvec, mu, trmx);% just 3 first PCA components, two classes & 20 samples per classtrain = tvec40(tlab == 1, 1:3)(1:20, :);train = [train; tvec40(tlab == 2, 1:3)(1:20, :)];trlab = [ones(20,1); 2*ones(20,1)];size(train)% shuffling of data is important herereorder = randperm(40);train = train(reorder, :);trlab = trlab(reorder);% let's create the network[hidl outl] = crann(columns(train), 5, 2);% and start serious work ...[hhidl ooutl ter] = backprop(train, trlab, hidl, outl, 0.1);clres = anncls(train, hhidl, ooutl)confMx(trlab, clres)% After implementing backprop I'm ready to prepare my reference network% remove columns with zero std toRemain = std(tvec) != 0;tvec = tvec(:, toRemain);tstv = tstv(:, toRemain);noHiddenNeurons = 200;noEpochs = 2;learningRate = 0.005;% break set into two subsets (50 thousand learning, 10 thousand testing)% in main loop training check recognition quality on validation set. if it starts% to go down (or no improvement), we should break, beacuse we are overfitting[hlnn olnn] = crann(columns(tvec), noHiddenNeurons, 10);totalError = zeros(1, noEpochs);trainError = zeros(1, noEpochs);trReport = [];for epoch=1:noEpochs	tic();	[hlnn olnn terr] = backprop(tvec, tlab, hlnn, olnn, learningRate);	clsRes = anncls(tvec, hlnn, olnn);	cfmx = confMx(tlab, clsRes);	errcf = compErrors(cfmx);	totalError(epoch) = terr;	trainError(epoch) = errcf(2);	epochTime = toc()	trReport = [trReport; epoch epochTime totalError(epoch) trainError(epoch)];	trReport(epoch, :)	fflush(stdout);end